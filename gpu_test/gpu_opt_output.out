------------------------------------
Configuration Information:
------------------------------------
Wed Nov 27 04:01:35 2024       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Tesla V100-PCIE-32GB           Off |   00000000:3B:00.0 Off |                    0 |
| N/A   29C    P0             36W /  250W |       0MiB /  32768MiB |      1%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   1  Tesla V100-PCIE-32GB           Off |   00000000:D8:00.0 Off |                    0 |
| N/A   31C    P0             37W /  250W |       0MiB /  32768MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
Date: Wed Nov 27 04:01:36 EST 2024
Time: 04:01:36
OS: Linux
Kernel: 3.10.0-514.2.2.el7.x86_64
Memory: 191844 MB
GPU Model: Tesla V100-PCIE-32GB
Tesla V100-PCIE-32GB
GPU Driver Version: 550.54.15
550.54.15
CUDA Version: V12.1.66  
Python Version: Python 3.9.20
NumPy Version: 1.26.4
PyTorch Version: 2.4.1
Project Directory: /work/jmalekar/MNSIM-2.0
PWD: /work/jmalekar/MNSIM-2.0/gpu_test
GPUs: 0
CPUS: 40
HOST: node368
Python: /work/jmalekar/MNSIM-2.0/gpu_test/env/bin/python
CONDA: /work/jmalekar/MNSIM-2.0/gpu_test/env
Partition: gpu-v100-32gb
Job ID: 17784730
User: jmalekar
Running command:
python test_gpu_llama.py

------------------------------------
------------------------------------

Profiling meta-llama/Llama-2-7b-hf
Error profiling meta-llama/Llama-2-7b-hf: 
LlamaTokenizer requires the SentencePiece library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


Profiling meta-llama/Llama-2-13b-hf
Error profiling meta-llama/Llama-2-13b-hf: CUDA out of memory. Tried to allocate 50.00 MiB. GPU 0 has a total capacity of 31.73 GiB of which 18.19 MiB is free. Including non-PyTorch memory, this process has 31.71 GiB memory in use. Of the allocated memory 31.41 GiB is allocated by PyTorch, and 2.36 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Elapsed Time for  stage (D:H:M:S:MS): 0:0:16:49:307
------------------------------------
------------------------------------

Error profiling OPT-66b: CUDA out of memory. Tried to allocate 162.00 MiB. GPU 0 has a total capacity of 31.73 GiB of which 64.19 MiB is free. Including non-PyTorch memory, this process has 31.67 GiB memory in use. Of the allocated memory 31.29 GiB is allocated by PyTorch, and 13.52 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Finished profiling OPT-66b
----------------------------------------
Results saved to opt_model_profiling_results.csv
Elapsed Time for  stage (D:H:M:S:MS): 0:0:50:43:706
------------------------------------
------------------------------------

