------------------------------------
Configuration Information:
------------------------------------
Wed Nov 27 17:38:42 2024       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Tesla V100-PCIE-32GB           Off |   00000000:3B:00.0 Off |                    0 |
| N/A   35C    P0             26W /  250W |       0MiB /  32768MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   1  Tesla V100-PCIE-32GB           Off |   00000000:D8:00.0 Off |                    0 |
| N/A   67C    P0            230W /  250W |   18401MiB /  32768MiB |     95%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    1   N/A  N/A    299258      C   python3                                     18398MiB |
+-----------------------------------------------------------------------------------------+
Date: Wed Nov 27 17:38:42 EST 2024
Time: 17:38:42
OS: Linux
Kernel: 3.10.0-514.2.2.el7.x86_64
Memory: 191844 MB
GPU Model: Tesla V100-PCIE-32GB
Tesla V100-PCIE-32GB
GPU Driver Version: 550.54.15
550.54.15
CUDA Version: V12.1.66  
Python Version: Python 3.9.20
NumPy Version: 1.26.4
PyTorch Version: 2.4.1
Project Directory: /work/jmalekar/MNSIM-2.0
PWD: /work/jmalekar/MNSIM-2.0/gpu_test
GPUs: 0
CPUS: 40
HOST: node377
Python: /work/jmalekar/MNSIM-2.0/gpu_test/env/bin/python
CONDA: /work/jmalekar/MNSIM-2.0/gpu_test/env
Partition: gpu-v100-32gb
Job ID: 17789985
User: jmalekar
Running command:
python test_gpu_llama.py

------------------------------------
------------------------------------

Profiling meta-llama/Llama-2-7b-hf
  Sequence length: 64
    Attention Layer Latency: 0.379123 seconds
    Overall Model Latency: 3.636830 seconds
  Sequence length: 127
    Attention Layer Latency: 0.031998 seconds
    Overall Model Latency: 0.038363 seconds
  Sequence length: 128
    Attention Layer Latency: 0.000610 seconds
    Overall Model Latency: 0.027243 seconds
  Sequence length: 256
    Attention Layer Latency: 0.000645 seconds
    Overall Model Latency: 0.034816 seconds
  Sequence length: 512
    Attention Layer Latency: 0.000655 seconds
    Overall Model Latency: 0.038261 seconds
  Sequence length: 1024
    Attention Layer Latency: 0.000614 seconds
    Overall Model Latency: 0.057265 seconds
  Sequence length: 2048
    Attention Layer Latency: 0.000424 seconds
    Overall Model Latency: 0.079497 seconds
  Sequence length: 4096
Error profiling meta-llama/Llama-2-7b-hf: CUDA out of memory. Tried to allocate 32.00 MiB. GPU 0 has a total capacity of 31.73 GiB of which 30.19 MiB is free. Including non-PyTorch memory, this process has 31.70 GiB memory in use. Of the allocated memory 31.01 GiB is allocated by PyTorch, and 328.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

Profiling meta-llama/Llama-2-13b-hf
Error profiling meta-llama/Llama-2-13b-hf: CUDA out of memory. Tried to allocate 136.00 MiB. GPU 0 has a total capacity of 31.73 GiB of which 80.19 MiB is free. Including non-PyTorch memory, this process has 31.65 GiB memory in use. Of the allocated memory 31.20 GiB is allocated by PyTorch, and 85.72 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Elapsed Time for  stage (D:H:M:S:MS): 0:0:2:56:243
------------------------------------
------------------------------------

